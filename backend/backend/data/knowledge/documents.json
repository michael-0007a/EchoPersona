{
  "doc_1915227154551702297_0": {
    "title": "Liu_DeepFashion_Powering_Robust_CVPR_2016_paper_1",
    "content": "1096 DeepFashion: Large-Scale Clothing Dataset  \n Ziwei Liu1 Ping Luo3,1 Shi Qiu2 Xiaogang Wang1,3 Xiaoou Tang1,3 \n1 2 3 \nThe Chinese University of Hong Kong  SenseTime Group Limited  Shenzhen Institutes of Advanced Technology, CAS  \n{lz013,pluo,xtang }@ie.cuhk.edu.hk, sqiu@sensetime.com, xgwang@ee.cuhk.edu.hk  \nAbstract \nRecent advances in clothes recognition have been driven \nby the construction of clothes datasets. Existing datasets \nare limited in the amount of annotations and are difficult to \ncope with the various challenges in real-world \napplications. In this work, we introduce DeepFashion1, a \nlarge-scale clothes dataset with comprehensive \nannotations. It contains over 800,000 images, which are \nrichly annotated with massive attributes, clothing \nlandmarks, and correspondence of images taken under \ndifferent scenarios including store, street snapshot, and \nconsumer. Such rich annotations enable the development of \npowerful algorithms in clothes recognition and facilitating \nfuture researches. To demonstrate the advantages of \nDeepFashion, we propose a new deep model, namely \nFashionNet, which learns clothing features by jointly \npredicting clothing attributes and landmarks. The estimated \nlandmarks are then employed to pool or gate the learned \nfeatures. It is optimized in an iterative manner. Extensive \nexperiments demonstrate the effectiveness of FashionNet \nand the usefulness of DeepFashion.  \n1. Introduction  \nRecently, extensive research efforts have been devoted to \nclothes classification [ 11, 1, 29], attribute prediction [ 3, 13, \n4, 24], and clothing item retrieval [ 17, 6, 10, 27, 15], \nbecause of their potential values to the industry. However, \nclothes recognition algorithms are often confronted with \nthree fundamental challenges when adopted in realworld \napplications [ 12]. First, clothes often have large variations \nin style, texture, and cutting, which confuse existing \nsystems. Second, clothing items are frequently subject to \ndeformation and occlusion. Third, clothing images often \nexhibit serious variations when they are taken under \ndifferent scenarios, such as selfies vs. online shopping \nphotos. \nPrevious studies tried to handle the above challenges by \nannotating clothes datasets either with semantic attributes  \n \n1 The dataset is available at: http://mmlab.ie.cuhk.edu.hk/ projects/DeepFashion.html   \nFigure 1. (a) Additional landmark locations improve clothes recognition.  \n(b) Massive attributes lead to better partition of the clothing feature space.  \n(e.g. color, category, texture) [ 1, 3, 6], clothing locations \n(e.g. masks of clothes) [ 20, 12], or cross-domain image \ncorrespondences [ 10, 12]. However, different datasets are \nannotated with different information. A unified dataset with \nall the above annotations is desired. This work fills in this \ngap. As illustrated in Fig. 1, we show that clothes \nrecognition can benefit from learning these annotations \njointly. In Fig. 1 (a), given the additional landmark locations \nmay improve recognition. As shown in Fig. 1 (b), massive \nattributes lead to better partition of the clothing feature \nspace, facilitating the recognition and retrieval of \ncrossdomain clothes images.  \nTo facilitate future researches, we introduce \nDeepFashion , a comprehensively annotated clothes dataset \nthat contains massive attributes, clothing landmarks, as well \nas cross-pose/cross-domain correspondences of clothing \npairs. This dataset enjoys several distinct advantages over \nits \n\n1097 precedents. (1) Comprehensiveness - images of \nDeepFashion are richly annotated with categories, \nattributes, landmarks, and cross-pose/cross-domain pair \ncorrespondences. It has 50 fine-grained categories and \n1,000 attributes, which are one order of magnitude larger \nthan previous works [ 3, 4, 10]. Our landmark annotation is \nat a finer level than existing bounding-box label [ 12]. Such \ncomprehensive and rich information are not available in \nexisting datasets. (2) Scale - DeepFashion contains over \n800K annotated clothing images, doubling the size of the \nlargest one in the literature. 3) Availability - DeepFashion \nwill be made public to the research community. We believe \nthis dataset will greatly benefits the researches in clothes \nrecognition and retrieval. \nMeanwhile, DeepFashion also enables us to rigorously \nbenchmark the performance of existing and future \nalgorithms for clothes recognition. We create three \nbenchmarks, namely clothing attribute prediction, in-shop \nclothes retrieval, and cross-domain clothes retrieval, a.k.a. \nstreet-toshop. With such benchmarks, we are able to make \ndirect comparisons between different algorithms and gain \ninsights into their pros and cons, which we hope will \neventually foster more powerful and robust clothes \nrecognition and retrieval systems. \nTo demonstrate the usefulness of DeepFashion, we \ndesign a novel deep learning structure, FashionNet, which \nhandles clothing deformation/occlusion by pooling/gating \nfeature maps upon estimated landmark locations. When \nsupervised by massive attribute labels, FashionNet learns \nmore discriminative representations for clothes recognition. \nWe conduct extensive experiments on the above \nbenchmarks. From the experimental results with the \nproposed deep model and the state-of-the-arts, we show that \nthe DeepFashion dataset promises more accurate and \nreliable algorithms in clothes recognition and retrieval. \nThis work has three main contributions. (1) We build a \nlarge-scale clothes dataset of over 800K images, namely \nDeepFashion, which is comprehensively annotated with \ncategories, attributes, landmarks, and cross-\npose/crossdomain pair correspondences. To our knowledge, \nit is the largest clothes dataset of its kind. (2) We develop FashionNet to jointly predict attributes and landmarks. The \nestimated landmarks are then employed to pool/gate the \nlearned features. It is trained in an iterative manner. (3) We \ncarefully define benchmark datasets and evaluation \nprotocols for three widely accepted tasks in clothes \nrecognition and retrieval. Through extensive experiments \nwith our proposed model as well as other state-of-thearts, \nwe demonstrate the effectiveness of DeepFashion and \nFashionNet. \n1.1. Related Work \nClothing Datasets As summarized in Table 1, existing \nclothes recognition datasets vary in size as well as the \namount of annotations. The previous datasets were labeled \nwith limited number of attributes, bounding boxes [ 12], or \nconsumer-to-shop pair correspondences [ 10]. DeepFashion \ncontains 800K images, which are annotated with 50 \ncategories, 1,000 attributes, clothing landmarks (each \nimage has 4 ∼ 8 landmarks), and over 300K image pairs. It \nis the largest and most comprehensive clothes dataset to \ndate. Some other datasets in the vision community were \ndedicated to the tasks of clothes segmentation, parsing [ 32, \n31, 23, 16, 33] and fashion modeling [ 24, 30], while \nDeepFashion focuses on clothes recognition and retrieval. \nClothing Recognition and Retrieval Earlier works [ 28, 3, \n7, 1, 6] on clothing recognition mostly relied on handcrafted \nfeatures, such as SIFT [ 19], HOG [5] and color histogram \netc. The performance of these methods were limited by the \nexpressive power of these features. In recent years, a \nnumber of deep models have been introduced to learn more \ndiscriminative representation in order to handle cross-\nscenario variations [ 10, 12]. Although these methods \nachieved good performance, they ignored the deformations \nand occlusions in the clothing images, which hinder further \nimprovement of the recognition accuracy. FashionNet \nhandles such difficulties by explicitly predicting clothing \nlandmarks and pooling features over the estimated \nlandmarks, resulting in more discriminative clothes \nrepresentation.  DCSA [3] ACWS [ 1] WTBI [ 12] DDAN [ 4] DARN [ 10] DeepFashion  \n# images 1856 145,718 78,958 341,021 182,780 >800,000 \n# categories + \nattributes 26 15 11 67 179 1,050 \n# exact pairs  N/A N/A 39,479 N/A 91,390 >300,000 \nlocalization  N/A N/A bbox N/A N/A 4∼8 landmarks  \npublic availability  yes yes no no no yes \nTable 1. Comparing DeepFashion with other existing datasets. DeepFashion offers the largest number of images and annotations.  \n1098 2. The DeepFashion Dataset  \nWe contribute DeepFashion, a large-scale clothes \ndataset, to the community. DeepFashion has several \nappealing properties. First, it is the largest clothing dataset \nto date, with over 800,000 diverse fashion images ranging \nfrom well-posed shop images to unconstrained consumer \nphotos, making it twice the size of the previous largest \nclothing dataset. Second, DeepFashion is annotated with \nrich information of clothing items. Each image in this \ndataset is labeled with 50 categories, 1,000 descriptive \nattributes, and clothing landmarks. Third, it also contains \nover 300,000 cross-pose/cross-domain image pairs. Some \nexample images along with the annotations are shown in \nFig.2. From the comparison items summarized in Table 1, \nwe see that DeepFashion surpasses the existing datasets in \nterms of scale, richness of annotations, as well as \navailability.  \n2.1. Image Collection  \nShopping websites are a common source for constructing \nclothing datasets [ 10, 12]. In addition to this source, we also \ncollect clothing images from image search engines, where \nthe resulting images come from blogs, forums, and the other \n \n2 www.forever21.com  \n3 www.mogujie.com  user-generated contents, which supplement and extend the \nimage set collected from the shopping websites.  \nCollecting Images from Shopping Websites We crawled \ntwo representative online shopping websites, Forever212 \nand Mogujie3. The former one contains images taken by the \nonline store. Each clothing item has 4 ∼ 5 images of varied \nposes and viewpoints. The latter one contains images taken \nby both the stores and consumers. Each clothing image in \nshop is accompanied by several user-taken photos of exactly \nthe same clothing item. Therefore, these data not only cover \nthe image distribution of professional online retailer stores, \nbut also the other different domains such as street snapshots \nand selfies. We collected 1,320,078 images of 391,482 \nclothing items from these two websites.  \nCollecting Images from Google Images4  To obtain \nmeaningful query keywords for clothing images, we \ntraversed the catalogue of several online retailer stores and \ncollected names of clothing items, such as “animal print \ndress”. This process results in a list of 12,654 unique \nqueries. We then feed this query set to Google Images, and \ndownload the returned images along with their associated \nmeta-data. A total of 1,273,150 images are collected from \nGoogle Images.  \n4 https://www.google.com/imghp   \nFigure 3. (a) Image number of the top- 20 categories. (b) Image number of the top- 10 attributes in each group.  \n\n1099 Data Cleaning We identified near- and exact-duplicate \nimages by comparing fc7-responses after feeding them into \nAlexNet [ 14]. After the removal of the duplicates, we ask \nhuman annotators to remove unusable images that are of \nlow resolution, image quality, or whose dominant objects \nare irrelevant to clothes. In total, 800,000 clothing images \nare kept to construct DeepFashion.  \n2.2. Image Annotation  \nWe advocate the following labeled information in order \nto aid the tasks for clothing recognition and retrieval. They \nare: (1) Massive attributes - this type of information is \nessential to recognize and represent the enormous clothing \nitems; (2) Landmarks - the landmark locations can \neffectively deal with deformation and pose variation; (3) \nConsumer-to-shop pairs - these data is of great help in \nbridging the cross-domain gap.  \nGenerating Category and Attribute Lists We generated \ncategory and attribute lists from the query set collected in \nSec.2.1, where most queries are of the form “adjective + \nnoun” (e.g. “animal print dress”). For clothing categories, \nwe first extracted the nouns (e.g. “dress”) from the query \nset, resulting in 50 unique names of fine-grained categories. \nNext, we collected and merged the adjectives ( e.g. “animal \nprint”), and picked the top 1,000 tags with highest \nfrequency as the attributes. These attributes were \ncategorized into five groups, characterizing texture, fabric, \nshape, part, and style, respectively.  \nCategory and Attribute Annotation The category set is of \nmoderate size ( i.e. 50) and the category labels are mutually \nexclusive by definition. Therefore, we instruct professional \nhuman annotators to manually assign them to the images. \nEach image received at most one category label. The \nnumbers of images for the top-20 categories are shown in \nFig.3 (a). As for the 1,000 attributes, since the number is \nhuge and multiple attributes can fire on the same image, \nmanual annotation is not manageable. We thus resort to the \nmeta-data for automatically assigning attribute labels. \nSpecifically, for each clothing image, we compare the \nattribute list with its associated meta-data, which is \nprovided by Google or corresponding shopping website. We \nregard an attribute as “fired” if it is successfully matched in \nthe image’s meta-data. We show sample images for a \nnumber of selected attributes in Fig. 2. We enumerated top \nten attributes in each group, along with their image numbers \nin Fig.3 (b). \nLandmark Annotation We define a set of clothing \nlandmarks, which corresponds to a set of key-points on the \nstructures of clothes. For instance, the landmarks for upper-\nbody items are defined as left/right collar end, left/right sleeve end, and left/right hem. Similarly, we define \nlandmarks for lower-body items and full-body items. As the \ndefinitions are straightforward and natural to average \npeople, the human labelers could easily understand the task \nafter studying a score of examples. As some of the \nlandmarks are frequently occluded in images, we also \nlabeled the visibility ( i.e. whether a landmark is occluded or \nnot) of each landmark. Note that our landmarks are \nclothescentric, and thus different from joints of human \nbody. Fig. 4 illustrates some examples of landmark \nannotations.  \nPair Annotation As discussed in Sec. 2.1, we collected \nclothing images under different domains, including photos \nfrom web stores, street snapshots, and consumers. We clean \nsuch image pairs by removing noisy images, ensuring the \naccuracy of our pairwise correspondences.  \nQuality Control We took the following steps to control \nthe labeling quality. (1) We discarded images with too few \ntextual meta-data. (2) After automatically annotating \nattributes, human annotators also conducted a fast screening  \n \nFigure 4. Landmarks and pair annotation in DeepFashion. Landmarks are \ndefined for upper-body clothes, lower-body clothes and full-body clothes, \nrespectively. Images in the same column capture the same clothing item.  \nto rule out falsely “fired” images for each attribute to ensure \nthe precision. (3) For other manually annotated labels, we \ncollected annotations from two different annotators and \ncheck their consistency. Around 0.5% samples were found \ninconsistent and required further labelling from a third \nannotator.  \n2.3. Benchmarks  \nWe build the following benchmarks out of DeepFashion \nfor evaluating different methods.  \nCategory and Attribute Prediction This task is to classify \n50 fine-grained categories and 1,000 attributes. There are \n63,720 diverse images in this benchmark. For category \nclassification, we employ the standard top- k classification \naccuracy as evaluation metric. For attribute prediction, our \nmeasuring criteria is the top- k recall rate following [ 9], \nwhich is obtained by ranking the 1,000 classification scores \nand determine how many attributes have been matched in \nthe top-k list. \n\n1100 \nIn-Shop Clothes Retrieval This task is to determine if \ntwo images taken in shop belong to the same clothing item \n5. It is important when customers encounter shop image on \nphoto sharing sites and would like to know more about its \nitem information on online retailer stores. This benchmark \ncontains 54,642 images of 11,735 clothing items from \nForever21 . Top-k retrieval accuracy is adopted to measure \nthe performance of fashion retrieval, such that a successful \nretrieval is counted if the exact fashion item has been found \nin the top- k retrieved results.  \nConsumer-to-Shop Clothes Retrieval This scenario has \nbeen considered by several previous works [ 10, 12], aiming \nat matching consumer-taken photos with their shop \ncounterparts. We select 251,361 consumer-to-shop image \npairs from Mogujie for this benchmark. Again, top- k \nretrieval accuracy is employed to evaluate performance.  \n3. Our Approach  \nTo demonstrate the usefulness of DeepFashion, we \npropose a novel deep model, FashionNet, which simultane-  \n \nFigure 5. Pipeline of FashionNet, which consists of global appearance \nbranch (in orange), local appearance branch (in green) and pose branch (in \nblue). Shared convolution layers are omitted for \nclarity. \nfeature maps  \n of conv4 landmark max-pooling \n... visibility 1 landmark  \nlocation 1landmark landmark max-pooling \nlocation  visibility  \nFigure 6. Schematic illustration of landmark pooling layer.  \nously predicts landmarks and attributes. The estimated \nlandmark locations are then employed to pool/gate the \nlearned features, inducing discriminative clothing features. \n \n5 We further annotate each image with its scale (zoom-in/zoom-out) \nand pose (front-view/side-view) using meta-data, which can be used for \nanalyzing the influence of different clothing variations.  This procedure performs in an iterative manner. And the \nwhole system can be learned end-to-end.  \nNetwork Structure The network structure of FashionNet \nis similar to VGG-16 [ 25], which has been demonstrated \npowerful in various vision tasks such as object recognition \n[25] and segmentation [ 18]. Specifically, the structures of \nFashionNet below the penultimate ( i.e. from top to bottom) \nconvolutional layer are the same as VGG-16, except the last \nconvolutional layer, which is carefully design for clothes. \nAs illustrated in Fig. 5, the last convolutional layer in VGG-\n16 is replaced by three branches of layers, highlighted in \nred, green, and blue respectively. The branch in red captures \nglobal features of the entire clothing item, while the branch \nin green captures local features pooling over the estimated \nclothing landmarks. The branch in blue predicts the \nlandmarks’ locations as well as their visibility ( i.e. whether \nthey have been occluded or not). Moreover, the outputs of \nthe branches in red and green are concatenated together as \nin “fc7 fusion” to jointly predict the clothes categories, \nattributes, and to model clothes pairs.  \nForward Pass The forward pass of FashionNet consists \nof three stages as shown in Fig. 5. At the first stage, a clothes \nimage is fed into the network and passed through the branch \nin blue, so as to predict the landmarks’ locations. At the \nsecond stage, the estimated landmarks are employed to pool \nor gate the features in “pool5 local”, which is a landmark \npooling layer , leading to local features that are invariant to \ndeformations and occlusions of clothes. At the third stage, \nthe global features of “fc6 global” and the landmark-pooled \nlocal features of “fc6 local” are concatenated together in \n“fc7 fusion”.  \nBackward Pass The backward pass back-propagates the \nerrors of four kinds of loss functions in an iterative manner. \nHere, we first introduce these loss functions and then \ndiscuss the iterative training strategy. These loss functions \ninclude a regression loss for landmark localization, a \nsoftmax loss for the predictions of landmark visibility and \nclothes categories, a cross-entropy loss for attribute \npredictions, and finally a triplet loss for metric learning of \nthe pairwise clothes images. First, a modified L2 regression \nloss is used to localize landmarks, Llandmarks = \n, where D, ℓˆj, and v j denote the  \nnumber of training samples, the ground truth locations of \nthe landmarks of the j-th sample, and a vector of its \nlandmarks’ visibility, respectively. Unlike the conventional \nregression loss, the visibility variables remedy missing \nground truth locations of the landmarks, in the sense that the \nx \ny \nlandmark  \nlocation \nlandmark  \nvisibility \n category \nattributes \ntriplet \nconv5_pose \nfc6_pose \nfc7_fusion \n fc7_pose \nconv5_global  \nfc6_local \n fc6_global \npool5_local \n1 \n2 \n3 \nfeature maps  \nof pool5_local  \n\n1101 error is not propagated back when a landmark is occluded. \nSecond, we adopt 1-of- K softmax loss to classify landmark \nvisibility and fine-grained categories, denoted as Lvisibility and \nLcategory respectively.  \nThird, a weighted cross-entropy loss is utilized to predict \nattributes \n(1) \n, \nwhere x j and aj represent the j-th clothes image and its \nattribute labels. wpos and wneg are two coefficients, \ndetermined by the ratio of the numbers of positive and \nnegative samples in the training set.  \nFourth, to learn the metric described by clothes pairs, we \nemploy triplet loss introduced in [ 22], which enforces \ndistance constraints among positive and negative samples  \n|D| \nLtriplet = Xmax{0 ,m + d(xj,x+j ) − d(xj,x−j )}, (2) j=1 \nwhere a three-tuple (x,x+,x−) is a triplet. x+ and x− indicate \nclothes images of the same and different item with respect \nto x. d(·,·) is a distance function and m is the margin \nparameter.  \nFashionNet is optimized by weighted combing the above \nloss functions. Here we discuss the iterative training \nstrategy that repeats the following two steps. In the first \nstep, we treat the branch in blue as the main task and the \nremaining branches as the auxiliary tasks. To this end, we \nassign Lvisibility and Llandmark with large weights, while the \nother loss functions have small weights. This is to train landmark estimation with the assistance of the other tasks, \nsince they are correlated. Joint optimization leads to  \nattributes. \nbetter convergence, which is demonstrated in Sec. 4.2. In the \nsecond step , we predict clothing categories and attributes, \nas well as to learn the pairwise relations between clothes \nimages. In this step, the estimated landmark locations are \nused to pool the local features. The above two steps are \niterated until convergence. This procedure is similar to [ 21]. \nLandmark Pooling Layer The landmark pooling layer is \na key component in FashionNet. Here, we discuss it in \ndetail. As shown in Fig. 6, the inputs of the landmark pooling \nlayer are the feature maps ( i.e. “conv4”) and the estimated \nlandmarks. For each landmark location ℓ, we first determine \nits visibility v. The responses of invisible landmark are \ngated to zero. Next, we perform max-pooling inside the \nregion around ℓ to obtain local feature maps. These local \nfeature maps are stacked to form the final feature maps of \n“pool5 local”. The back-propagation of the landmark \npooling layer is similar to the RoI pooling layer introduced \nin [8]. However, unlike [ 8] that treated the pooled regions \nindependently, the landmark pooling layer captures \ninteraction between clothing landmarks by concatenating \nlocal features.  \n4. Experiments  \nData We pre-train FashionNet on a subset of 300,000 \nimages of DeepFashion, another subset of 50,000 images is \nused as validation data. In testing, we employ part of the \nbenchmark data to fine-tune the pre-trained models on the \nthree benchmarks. We ensure that no fashion item overlaps \nbetween fine-tuning and testing sets.  Figure 7. Per-attribute prediction performance for 70 representative attributes. FashionNet consistently outperforms WTBI [ 12] and DARN [ 10] on all \n\n1102 Competing Methods We compare FashionNet with two \nrecent deep models that showed compelling performance in \nclothes recognition, including Where To Buy It (WTBI) \n[12] and Dual Attribute-aware Ranking Network (DARN) \n[10]. Both of them are trained using clothes bounding \nboxes. Specifically, WTBI concatenated multi-layer \nperceptron (MLP) on top of the pre-trained ImageNet \nmodels [25]. We only implement the category-independent \nmetric network of WTBI, which handles all clothing \ncategories in a single network. DARN adopted an attribute-\nregularized two-stream CNN. One stream handles shop \nimages, while the other handles street images. Note that for \ncategory classification and attribute prediction, only one \nstream of DARN is used. We train WTBI and DARN using \nthe same amount of data and protocol as FashionNet did.  \nWe also vary building blocks of FashionNet for an \nablation study, including FashionNet+100 and \nFashionNet+500. They represent that we only utilize 100 \nand \n500 attributes to learn FashionNet respectively, instead of \n1,000 attributes used in the full model. Next, we replace \nfashion landmarks in our model with detected human joints \n[34] and poselets [ 2] to pool/gate features in the stages of \ntraining and test. They are denoted as FashionNet+Joints \nand FashionNet+Poselets, respectively.  \n4.1. Results \nThis section provides quantitative evaluations of \ndifferent methods on the three benchmarks. We also \ninvestigate multiple building blocks of the proposed \nFashionNet. Table 2 summarizes the performance of \ndifferent methods on category classification and attribute \nprediction.  \nCategory Classification In fine-grained category \nclassification, we have three observations. First, \nFashionNet significantly outperforms WTBI and DARN by \n20 percent when evaluated using the top- 3 accuracy. It \noutperforms them by 10 percent in the top- 5 accuracy. \nPlease refer to Sec. 2.3 for the details of the evaluation \nmetrics of the benchmark. These results show that by adding \ninformative landmarks, FashionNet can better discover \nfine-grained clothing traits than existing deep models. Second, when replacing the clothing landmarks in \nFashionNet with human joints and poselets, 6 ∼9 percent \nperformance drops are observed. As the clothing landmarks \nare defined based on domain-specific semantics of clothes, \nthey are more effective than human joints/poselets in \nclothes recognition. Third, using massive attributes benefits \nfine-grained category classification. When training \nFashionNet with different number of attributes, including \n100, 500, and 1,000, the classification accuracies \nsignificantly increase. The full model surpasses \nFashionNet-500 and -100 by 13 and 20 percent in the top- 5 \naccuracy respectively, showing that richer attribute \ninformation helps comprehensive profiling of different \nclothing variations.  \nAttribute Prediction For attribute prediction, similar \nconclusions can be drawn for the effectiveness of \ninformative landmarks and massive attributes. To \nunderstand the strength of FashionNet, we also present the \nattribute recall results for each of the five attribute groups. \nWe demonstrate that FashionNet achieves compelling \nresults in the attribute groups of “shape” and “part”, because \ndiscriminative information of these attributes normally exist \naround clothing landmarks, thus can be well captured by \nlandmark pooling in FashionNet. Fig. 7 illustrates the per-\nattribute recall rates of top-5 accuracy for the 70 \nrepresentative attributes. Our approach consistently \noutperforms WTBI and DARN on all of the attributes in this \nbenchmark.  \nIn-Shop Clothes Retrieval Fig. 8 shows the top- k retrieval \naccuracy of all the compared methods with k ranging from \n1 to 50. We also list the top-20 retrieval accuracy after the \nname of each method. We can clearly see that our model \n(FashionNet) achieves best performance (0.764) among all \nthe methods under comparison, while WTBI has the lowest \naccuracy (0.506). The poor performance of WTBI is as \nexpected, since it directly used the pre-trained ImageNet \nfeatures, which are not suitable to describe clothes. Notably, \ncompared with DARN, FashionNet boots the top-20 \naccuracy from 0.675 to 0.764, and a 15 percent relative \nimprovement is attained. This reveals the merits of \nemploying landmarks to pool and gate learned features.  \n \nFigure 8. Results on in-shop clothes retrieval benchmark. (a) Example queries, top- 5 retrieved images, along with their predicted landmarks. Correct \nmatches are marked in green. (b) Retrieval accuracies of different methods under comparison.  \n\n1103 When we replace clothing landmarks with human joints \n(FashionNet+Joints) or poselets (FashionNet+Poselets), the \naccuracy drops by 8 and 6 percent respectively, indicating \nsuch options are suboptimal. Compared with \nFashionNet+100 and FashionNet+500, FashionNet increase \nthe accuracy by 19 and 12 percent, respectively, which \nhighlights the effectiveness of using massive clothing \nattributes for training deep models. Some of the sample \nresults are given in Fig. 8 (a), where top retrieved images \nalong with predicted landmark locations are shown.  \nConsumer-to-Shop Clothes Retrieval We show the \ndetailed retrieval accuracy of different methods in Fig. 9 (b). \nCompared with in-shop retrieval, methods on this \nbenchmark achieve much lower accuracies, which reflect \nthe inherent difficulty of consumer-to-shop clothes \nretrieval. Similar to in-shop clothes retrieval, FashionNet \nachieves the best top-20 accuracy ( i.e. 0.188) among \ndifferent methods. The relative improvement of FashionNet \nover DARN rises to 70 percent, compared to 15 percent of \nthe previous benchmark, indicating the landmark locations \nare of greater importance for more complex scenarios. \nBesides, the retrieval accuracy increases when more \ntraining attributes are explored. Moreover, using human \nlandmarks rather than clothing landmarks degrades the \naccuracy. These observations are consistent with those in \nthe previous task. Some sample queries along with their top \nmatches are shown in Fig. 9 (a). 4.2. Further Analysis  \nAs landmark estimation plays a key role in FashionNet, \nwe conducted a quantitative evaluation of this component \nin order to better understand our method. For a more \ndetailed analysis of search results, we also explore how \ndifferent variations of clothes affect the retrieval accuracy.  \nLandmark Estimation Fig. 10 (a) illustrates the detection \nrates over varying thresholding distances for different \nclothing landmarks. Similar to [ 26], percentage of detected  \nFigure 10. (a) Detection rates of different clothing landmarks. (b) \nDetection rates with and without using attributes.  \njoints (PDJ) is utilized to evaluate landmark estimation. \nWhen the normalized distance equals 0.1, the detection \nrates are above 80 percent for all the eight landmarks. We \ncan further observe that detection rate of collars are higher \nthan that of sleeves, waistlines, and hems. This is because \ncollars are relatively rigid w.r.t. human’s neck, whereas \nsleeves, waistlines, and hems are more flexible beyond \ncommon human joints. Fig. 10 (b) demonstrates that rich \nattribute information facilitates landmark localization, \nbecause some attributes can effectively describe the \nappearance of certain clothing landmarks, such as “cap-\nsleeve” and “fringed-hem”.  \nVariations of Clothes We choose the in-shop clothes \nretrieval benchmark to investigate the influence of different \nvariations of clothes. Fig. 11 (a) illustrates the retrieval \nperformance of query images with different variations. We \ncan see that scale variations are more challenging than pose \nvariations. Another interesting observation is that zoom-in  \nFigure 9. Results on consumer-to-shop clothes retrieval benchmark. (a) Example queries, top- 3 retrieved images, along with their predicted landmarks. \nCorrect matches are marked in green. (b) Retrieval accuracies of different methods under comparison.  \n\n1104 images perform worse than zoom-out images when k = 1, \nhowever its performance increases when k gets larger. It is \nbecause landmarks are essential for accurate fashion \nretrieval, but they are undetectable in zoom-in images. The \nfine-grained texture attributes help recognize zoom-in \nimages and may guarantee an acceptable retrieval \nperformance when k gets large. From Fig. 11 (b), we can \nobserve that “dress” has the highest accuracy while “shorts” \nhas the lowest, because “dresses” generally have much \nmore distinguishable features, such as local traits and \ncolors. “Shorts”, on the other hand, tend to have similar \nFigure 11. (a) Retrieval accuracies under different poses and scales. (b) \nRetrieval accuracies of different clothing categories.  \nshape and relatively plain textures. \n5. Conclusions \nThis work presents DeepFashion, a large-scale clothing \ndataset with comprehensive annotations. DeepFashion \ncontains over 800,000 images, which are richly labeled with \nfine-grained categories, massive attributes, landmarks, and \ncross-pose/cross-domain image correspondence. It \nsurpasses existing clothing datasets in terms of scale as well \nas richness of annotation. To demonstrate the advantages of \nsuch comprehensive annotations, we designed a novel deep \nmodel, namely FashionNet, that learns clothing features by \njointly predicting landmark locations and massive \nattributes. The estimated landmarks are used to pool or gate \nthe learned feature maps, which leads to robust and \ndiscriminative representations for clothes. We establish \nbenchmark datasets for three widely accepted tasks in \nclothing recognition and retrieval. Through extensive \nexperiments, we demonstrate the effectiveness of \nFashionNet and the usefulness of DeepFashion, which may \nsignificantly facilitate future researches. \nAcknowledgement This work is partially supported by \nSenseTime Group Limited, the Hong Kong Innovation and \nTechnology  \nSupport Programme (No. ITS/121/15FX), the General Research  \nFund sponsored by the Research Grants Council of Hong Kong \n(Project Nos. CUHK14203015, CUHK14207814), and the \nNational Natural Science Foundation of China (61503366).  \nReferences \n[1] L. Bossard, M. Dantone, C. Leistner, C. Wengert, T. Quack, \nand L. Van Gool. Apparel classification with style. In ACCV, \npages 321–335. 2012. 1, 2 \n[2] L. Bourdev and J. Malik. Poselets: Body part detectors \ntrained using 3d human pose annotations. In ICCV, pages \n1365–1372, 2009. 6 [3] H. Chen, A. Gallagher, and B. Girod. Describing clothing by \nsemantic attributes. In ECCV, pages 609–623. 2012. 1, 2, 6 \n[4] Q. Chen, J. Huang, R. Feris, L. M. Brown, J. Dong, and S. \nYan. Deep domain adaptation for describing people based on \nfine-grained clothing attributes. In CVPR, pages 5315– \n5324, 2015. 1, 2 \n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for \nhuman detection. In CVPR, pages 886–893, 2005. 2 \n[6] W. Di, C. Wah, A. Bhardwaj, R. Piramuthu, and N. \nSundaresan. Style finder: Fine-grained clothing style \ndetection and retrieval. In CVPR Workshops , pages 8–13, \n2013. 1, 2 \n[7] J. Fu, J. Wang, Z. Li, M. Xu, and H. Lu. Efficient clothing \nretrieval with semantic-preserving visual phrases. In ACCV, \npages 420–431. 2012. 2 \n[8] R. Girshick. Fast r-cnn. In ICCV, 2015. 6 \n[9] Y. Gong, Y. Jia, T. Leung, A. Toshev, and S. Ioffe. Deep \nconvolutional ranking for multilabel image annotation. arXiv \npreprint arXiv:1312.4894 , 2013. 4 \n[10] J. Huang, R. S. Feris, Q. Chen, and S. Yan. Cross-domain \nimage retrieval with a dual attribute-aware ranking network. \nIn ICCV, 2015. 1, 2, 3, 4, 6 \n[11] Y. Kalantidis, L. Kennedy, and L.-J. Li. Getting the look: \nclothing recognition and segmentation for automatic product \nsuggestions in everyday photos. In ICMR, pages 105–112, \n2013. 1 \n[12] M. H. Kiapour, X. Han, S. Lazebnik, A. C. Berg, and T. L. \nBerg. Where to buy it: Matching street clothing photos in \nonline shops. In ICCV, 2015. 1, 2, 3, 4, 6 \n[13] M. H. Kiapour, K. Yamaguchi, A. C. Berg, and T. L. Berg. \nHipster wars: Discovering elements of fashion styles. In \nECCV, pages 472–488. 2014. 1 \n[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet \nclassification with deep convolutional neural networks. In \nNIPS, pages 1097–1105, 2012. 3 \n[15] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan. \nClothes co-parsing via joint image segmentation and \nlabeling with application to clothing retrieval. In IEEE \nTransactions on Multimedia , 2016. 1 \n[16] S. Liu, J. Feng, C. Domokos, H. Xu, J. Huang, Z. Hu, and S. \nYan. Fashion parsing with weak color-category labels. TMM, \n16(1):253–265, 2014. 2 \n[17] S. Liu, Z. Song, G. Liu, C. Xu, H. Lu, and S. Yan. Street-\ntoshop: Cross-scenario clothing retrieval via parts alignment \nand auxiliary set. In CVPR, pages 3330–3337, 2012. 1 \n[18] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic image \nsegmentation via deep parsing network. In ICCV, 2015. 5 \n[19] D. G. Lowe. Distinctive image features from scale-invariant \nkeypoints. IJCV, 60(2):91–110, 2004. 2 \n[20] P. Luo, X. Wang, and X. Tang. Pedestrian parsing via deep \ndecompositional network. In ICCV, pages 2648–2655, 2013. \n1 \n1105 [21] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards \nreal-time object detection with region proposal networks. In \nNIPS, 2015. 6 \n[22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A \nunified embedding for face recognition and clustering. In \nCVPR, 2015. 5 \n[23] E. Simo-Serra, S. Fidler, F. Moreno-Noguer, and R. Urtasun. \nA high performance crf model for clothes parsing. In ACCV, \n2014. 2 \n[24] E. Simo-Serra, S. Fidler, F. Moreno-Noguer, and R. Urtasun. \nNeuroaesthetics in fashion: Modeling the perception of \nbeauty. In CVPR, 2015. 1, 2 \n[25] K. Simonyan and A. Zisserman. Very deep convolutional \nnetworks for large-scale image recognition. arXiv preprint \narXiv:1409.1556 , 2014. 5, 6 \n[26] A. Toshev and C. Szegedy. Deeppose: Human pose \nestimation via deep neural networks. In CVPR, pages 1653– \n1660, 2014. 7 \n[27] A. Veit, B. Kovacs, S. Bell, J. McAuley, K. Bala, and S. \nBelongie. Learning visual clothing style with heterogeneous \ndyadic co-occurrences. In ICCV, pages 4642–4650, 2015. 1 \n[28] X. Wang and T. Zhang. Clothes search in consumer photos \nvia color matching and attribute learning. In ACM MM , \npages 1353–1356, 2011. 2 \n[29] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning \nfrom massive noisy labeled data for image classification. In \nCVPR, pages 2691–2699, 2015. 1 \n[30] K. Yamaguchi, T. L. Berg, and L. E. Ortiz. Chic or social: \nVisual popularity analysis in online fashion networks. In \nACM MM , pages 773–776, 2014. 2 \n[31] K. Yamaguchi, M. H. Kiapour, and T. Berg. Paper doll \nparsing: Retrieving similar styles to parse clothing items. In \nICCV, pages 3519–3526, 2013. 2 \n[32] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg. \nParsing clothing in fashion photographs. In CVPR, pages \n3570–3577, 2012. 2 \n[33] W. Yang, P. Luo, and L. Lin. Clothing co-parsing by joint \nimage segmentation and labeling. In CVPR, pages 3182– \n3189, 2014. 2 \n[34] Y. Yang and D. Ramanan. Articulated pose estimation with \nflexible mixtures-of-parts. In CVPR, pages 1385–1392,  \n2011. 6",
    "categories": [
      "support"
    ],
    "created_at": "2024-01-01T00:00:00",
    "word_count": 5747
  }
}